{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, OpenAI, ConversationChain\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import HNLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage, Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain.chains import SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SERPAPI_API_KEY\"] = \"f985eefafad24faf8cd967633691a0b77708747074984a989e05b9f31725a167\"\n",
    "pinecone_api_key = \"28810ba1-baa2-4b2b-a155-e77211a8dcc6\"\n",
    "openai_api_key = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "     Act as an AI assistant called alexis. You will first present yourself.\n",
    "     Your will get data for article on tokenomics, specifically on {input1}. Rewrite it:\n",
    "     \n",
    "     this is the data: {theory}\n",
    "         \n",
    "  \n",
    "     \n",
    "    \n",
    "    YOUR RESPONSE:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0.7,\n",
    "    openai_api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"serpapi\", \"llm-math\", \"wikipedia\",\"terminal\", ],\n",
    "    llm=llm,\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('/workspaces/langchain/Tokeby/Theory/tokenomics.txt')\n",
    "doc = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
    "docs = text_splitter.split_documents(doc)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = ChatOpenAI(temperature=0.6, openai_api_key=openai_api_key), chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction:\\n\\nThe internet has come a long way since its inception. From the early days of the client-server model to the current era of social media and cloud computing, the internet has transformed the way we live and work. However, the current model of the internet is far from perfect. It suffers from several issues such as the lack of transparency, the lack of privacy, and the reliance on centralized platforms. Web3 is the next stage of the internet, which promises to address many of the issues that plague the current model. In this article, we will explore what web3 is, how it works, and its potential implications.\\n\\nWhat is Web3?\\n\\nWeb3 is a term that refers to the next generation of the internet. It promises to be more decentralized, more secure, and more transparent than the current model. At its core, web3 is built on blockchain technology, which allows for the creation of decentralized applications (dApps) that run on a peer-to-peer network. Web3 is often referred to as the decentralized web, as it is designed to be more resilient and less vulnerable to attacks than the current model.\\n\\nHow Does Web3 Work?\\n\\nWeb3 is built on a decentralized network that is powered by blockchain technology. Blockchain is a distributed ledger that allows for secure and transparent transactions without the need for intermediaries. In the context of web3, blockchain is used to create decentralized applications that run on a peer-to-peer network. These applications are designed to be more transparent and secure than their centralized counterparts.\\n\\nWeb3 is designed to be more decentralized than the current model of the internet. This means that there is no central authority that controls the network. Instead, the network is powered by a decentralized network of nodes that work together to validate transactions and maintain the integrity of the network. This makes web3 more resilient to attacks and less vulnerable to censorship.\\n\\nWeb3 is also designed to be more transparent than the current model of the internet. This means that users have more control over their data and can choose how it is used. In the current model of the internet, users often have little control over their data and are forced to rely on centralized platforms to protect it. With web3, users have more control over their data and can choose how it is used.\\n\\nImplications of Web3:\\n\\nWeb3 has the potential to transform the way we live and work. It promises to be more decentralized, more transparent, and more secure than the current model of the internet. This has several implications for various industries.\\n\\nFinance:\\n\\nWeb3 has the potential to disrupt the finance industry. With the advent of decentralized finance (DeFi) applications, users can now access financial services without the need for intermediaries. This means that users can lend, borrow, and invest without the need for traditional financial institutions. This has the potential to make financial services more accessible and affordable for people around the world.\\n\\nGovernance:\\n\\nWeb3 has the potential to transform the way we govern ourselves. With the advent of decentralized autonomous organizations (DAOs), users can now participate in decision-making processes without the need for intermediaries. This means that users can vote on important issues and make decisions that affect their lives without the need for traditional political institutions. This has the potential to make governance more democratic and transparent.\\n\\nSupply Chain:\\n\\nWeb3 has the potential to transform the way we track and trace goods in the supply chain. With the advent of blockchain-based supply chain solutions, users can now track goods from the point of origin to the point of consumption. This means that users can verify the authenticity of goods and ensure that they are ethically sourced. This has the potential to make supply chains more transparent and ethical.\\n\\nConclusion:\\n\\nWeb3 is the next stage of the internet, which promises to be more decentralized, more transparent, and more secure than the current model of the internet. It is built on blockchain technology, which allows for the creation of decentralized applications that run on a peer-to-peer network. Web3 has the potential to transform various industries, including finance, governance, and supply chain. While there are still several challenges that need to be addressed, such as scalability and usability, the potential of web3 is immense.\\n\\nSources:\\n\\n1. Olpinski Maciej, 2016, Nov 2, Building ‘Google For The Economic Web’ on The Ethereum Blockchain, Medium Blog Post. Blockchain.https://blog.userfeeds.io/building-google-for-theeconomic-web-on-the-ethereum-blockchain-de27cb3d23b#.ski5jhoye\\n\\n2. Paulin, A.: “Through liquid democracy to sustainable non-bureaucratic government,” Proceedings of the International Conference for E-Democracy and Open Government, 205-217. 2014\\n\\n3. Roberts, Jeff John; Rapp, Nicholas: “Nearly 4 Million Bitcoins Lost Forever, New Study Says”, November 25, 2017, retrieved from: https://fortune.com/2017/11/25/lost-bitcoins/\\n\\n4. Sharma, Rakesh: “What Is Ethereum‘s „Difficulty Bomb“?”, Invesopedia, Aug 10, 2018: https://www.investopedia.com/news/what-ethereums-difficulty-bomb/\\n\\n5. Toffler, A.: ”The rise of the prosumer: The third wave,” New York: Bantam Books. 1984\\n\\n6. Veblen, T.: “Why is Economics Not an Evolutionary Science,” The Quarterly Journal of Economics, 12., 1898\\n\\n7. Virtanen, Akseli; Bryan, Dick; Lee, Benjamin; Wosnitzer, Robert: “Economics Back in Cryptoeconomics,” Sep 11 2018, retrieved from: https://medium.com/econaut/economics-back-into-cryptoeconomics-20471f5ceeea\\n\\n8. Voshmgir, Shermin: “Disrupting governance with blockchains and smart contracts”, Journal for Strategic Change, Special Issue: The Future of Money and Further Applications of the Blockchain, Volume 26, Issue 5, September 2017, Pages 499-509.\\n\\n9. Walch, Angela: “The Bitcoin Blockchain as Financial Market Infrastructure: A Consideration of Operational Risk.” New York University Journal of Legislation and Public Policy, 18: 837. 2015\\n\\n10. Walch, Angela: “In Code (Rs) We Trust: Software Developers as Fiduciaries in Public Blockchains,” 2019\\n\\n11. Werbach, Kevin. 2018. “Trust, but verify: Why the blockchain needs the law.” Berkeley Tech. LJ, 33: 487\\n\\n12. Weber Max, 1948, Essays in Sociology, translated, edited and with an introduction by H. H. Gerth and C. W. Mills. London: Routledge and Kegan Paul.\\n\\n13. Weber Max, 1978/1922, Economy and Society, edited by Guenther Roth and Claus Wittich. Berkeley: University of California Press.\\n\\n14. Walton H. Hamilton (1919). „The Institutional Approach to Economic Theory,“ American Economic Review, 9(1), Supplement, pp. 309–18. Reprinted in R. Albelda, C. Gunn, and W. Waller (1987), Alternatives to Economic Orthodoxy: A Reader in Political Economy, pp. 204-12.\\n\\n15. Wiener, Norbert: ”Cybernetics or Control and Communication in the Animal and the Machine,” Vol. 25, MIT press,1965.\\n\\n16. Williamson, Oliver:'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a 5000 character article on the concept of token economy\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 13:22:59.324 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/vscode/langchain-py-env/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import streamlit as st\n",
    "\n",
    "def get_text():\n",
    "    input_text = st.text_area(label=\"Description\", label_visibility='collapsed', placeholder=\"Let's get it...\", key=\"description_input\")\n",
    "    return input_text\n",
    "\n",
    "input1 = get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_button = \"\"\"\n",
    "<button type=\"button\" class=\"btn btn-secondary\" style=\"background-color: #3b82f6; color: white; font-weight: bold;\">See An Example Broo</button>\n",
    "\"\"\"\n",
    "st.markdown(example_button, unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"### Output:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input1': input1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (rest of the code stays the same)\n",
    "\n",
    "input1 = get_text()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "example_button = \"\"\"\n",
    "<button type=\"button\" class=\"btn btn-secondary\" style=\"background-color: #3b82f6; color: white; font-weight: bold;\">See An Example Broo</button>\n",
    "\"\"\"\n",
    "st.markdown(example_button, unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"### Output:\")\n",
    "\n",
    "#####################################################################\n",
    "input1= \"Write an article on tokenomics\"\n",
    "if input1:\n",
    "    query = input1\n",
    "    theory = qa.run(query)\n",
    "#####################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input1': input1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m input1:\n\u001b[0;32m----> 2\u001b[0m     chat_prompt_template1 \u001b[39m=\u001b[39m ChatPromptTemplate(\n\u001b[1;32m      3\u001b[0m         message\u001b[39m=\u001b[39;49mtemplate1,\n\u001b[1;32m      4\u001b[0m         input_variables\u001b[39m=\u001b[39;49m input1, \u001b[39m# Added this line\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m         partial_variables\u001b[39m=\u001b[39;49m theory, \u001b[39m# Added this line\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m     ) \u001b[39m# Corrected this line\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     chat \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      8\u001b[0m     chain1 \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mchat, prompt\u001b[39m=\u001b[39mchat_prompt_template1)\n",
      "File \u001b[0;32m~/langchain-py-env/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/langchain-py-env/lib/python3.11/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[1;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/langchain-py-env/lib/python3.11/site-packages/langchain/prompts/base.py:127\u001b[0m, in \u001b[0;36mBasePromptTemplate.validate_variable_names\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@root_validator\u001b[39m()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_variable_names\u001b[39m(\u001b[39mcls\u001b[39m, values: Dict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate variable names do not include restricted names.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m values[\u001b[39m\"\u001b[39;49m\u001b[39minput_variables\u001b[39;49m\u001b[39m\"\u001b[39;49m]:\n\u001b[1;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot have an input variable named \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, as it is used internally,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m please rename.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m values[\u001b[39m\"\u001b[39m\u001b[39mpartial_variables\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_variables'"
     ]
    }
   ],
   "source": [
    "if input1:\n",
    "    chat_prompt_template1 = ChatPromptTemplate(\n",
    "        message=template1,\n",
    "        input_variables= input1, # Added this line\n",
    "        partial_variables= theory, # Added this line\n",
    "    ) # Corrected this line\n",
    "    chat = ChatOpenAI(temperature=0.9)\n",
    "    chain1 = LLMChain(llm=chat, prompt=chat_prompt_template1)\n",
    "    response = chain1.run()\n",
    "    st.write(response)\n",
    "    response = qa.run(query)\n",
    "    st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1= \"Write an article on tokenomics\"\n",
    "if input1:\n",
    "    query = input1\n",
    "    theory = qa.run(query)\n",
    "if input1:\n",
    "    human_message_prompt1 = HumanMessagePromptTemplate(prompt={\"prompt\": template1}) # Corrected this line\n",
    "    chat_prompt_template1 = ChatPromptTemplate.from_messages([human_message_prompt1])\n",
    "    chat = ChatOpenAI(temperature=0.9)\n",
    "    chain1 = LLMChain(llm=chat, prompt=chat_prompt_template1)\n",
    "    response = chain1.run(inputs)\n",
    "    st.write(response)\n",
    "    response = qa.run(query)\n",
    "    st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HumanMessagePromptTemplate\nprompt\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m input1:\n\u001b[0;32m----> 2\u001b[0m     human_message_prompt1 \u001b[39m=\u001b[39m HumanMessagePromptTemplate(prompt\u001b[39m=\u001b[39;49mtemplate1)\n\u001b[1;32m      3\u001b[0m     chat_prompt_template1 \u001b[39m=\u001b[39m ChatPromptTemplate\u001b[39m.\u001b[39mfrom_messages([human_message_prompt1])\n\u001b[1;32m      4\u001b[0m     chat \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "File \u001b[0;32m~/langchain-py-env/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HumanMessagePromptTemplate\nprompt\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "if input1:\n",
    "    human_message_prompt1 = HumanMessagePromptTemplate(prompt=template1)\n",
    "    chat_prompt_template1 = ChatPromptTemplate.from_messages([human_message_prompt1])\n",
    "    chat = ChatOpenAI(temperature=0.9)\n",
    "    chain1 = LLMChain(llm=chat, prompt=chat_prompt_template1)\n",
    "    response = chain1.run(inputs)\n",
    "    st.write(response)\n",
    "    response = qa.run(query)\n",
    "    st.write(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-py-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
